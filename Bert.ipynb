{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4LcnDAMcWxYf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab, embedding_dim, position_size):\n",
        "    super().__init__()\n",
        "    self.embeddings = nn.Embedding(vocab,embedding_dim)\n",
        "    self.positions = nn.Embedding(position_size, embedding_dim)\n",
        "    self.norm = nn.LayerNorm(embedding_dim, eps=1e-12)\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    seq_length = input_ids.size(1)\n",
        "    position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
        "    token_embeddings = self.embeddings(input_ids)\n",
        "    position_embeddings = self.position(position_ids)\n",
        "    all_embeddings = token_embeddings + position_embeddings\n",
        "    all_embeddings = self.norm(all_embeddings)\n",
        "    all_embeddings = self.dropout(all_embeddings)\n",
        "    return all_embeddings #tokens * embedim"
      ],
      "metadata": {
        "id": "9fGAPcYspiAW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v):\n",
        "  dim_k = k.size(-1)\n",
        "  scores = torch.bmm(q, torch.transpose(k,1,2)/dim_k) # tokens * tokens\n",
        "  softed = F.softmax(scores, dim=-1) # tokens * tokens\n",
        "  attn_head = torch.bmm(softed, v) #tokens*embedding_dim\n",
        "  return attn_head"
      ],
      "metadata": {
        "id": "kEw9YctrX1v2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedding_dim, head_dim):\n",
        "    self.q = nn.Linear(embedding_dim, head_dim)\n",
        "    self.k = nn.Linear(embedding_dim, head_dim)\n",
        "    self.v = nn.Linear(embedding_dim, head_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return scaled_dot_product(self.q(x), self.k(x), self.v(x))"
      ],
      "metadata": {
        "id": "2JYsKOm6juBb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim, num_attention_heads):\n",
        "    head_dim = embedding_dim // num_attention_heads\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(embedding_dim, head_dim) for _ in range(num_attention_heads)]\n",
        "    )\n",
        "    self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "     out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "     out = self.fc(out)\n",
        "     return out"
      ],
      "metadata": {
        "id": "wPOH5VP6lDeT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedding_dim, intermediate_dim):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(embedding_dim, intermediate_dim)\n",
        "    self.l2 = nn.Linear(intermediate_dim, embedding_dim)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xDIjQKMXnny4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, embedding_dim, intermediate_dim, num_attention_heads):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.LayerNorm(embedding_dim)\n",
        "    self.l2 = nn.LayerNorm(embedding_dim)\n",
        "    self.attention = MultiHeadAttention(embedding_dim, num_attention_heads)\n",
        "    self.feed_forward = FeedForward(embedding_dim, intermediate_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    x = x + self.attention(out)\n",
        "    x = x + self.feed_forward(self.l1(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "bKDQhLNRoaUE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyBert(nn.Module):\n",
        "  def __init__(self,  vocab=30522, embedding_dim=768, position_size=512, n_layers=12,intermediate_dim=3072, num_attention_heads=12):\n",
        "    super().__init__()\n",
        "    self.embeddings = Embeddings(vocab, embedding_dim, position_size)\n",
        "    self.layers = nn.ModuleList([Encoder(embedding_dim, intermediate_dim, num_attention_heads) for _ in range(n_layers)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embeddings(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "nP0L5caDpdEy"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}